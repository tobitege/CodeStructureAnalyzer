# LLM Provider Configuration
LLM_PROVIDER="lmstudio"  # Options: "lmstudio" or "ollama"

# LMStudio Configuration (only used when LLM_PROVIDER="lmstudio")
LMSTUDIO_HOST="localhost:1234"

# Ollama Configuration (only used when LLM_PROVIDER="ollama")
OLLAMA_HOST="localhost:11434"
OLLAMA_MODEL="qwen2.5-coder:14b"

# Analysis Configuration
CHUNK_SIZE=200
OUTPUT_FILE="trace_ai.md"

# File Extensions to Analyze (comma-separated)
FILE_EXTENSIONS=".cs,.py,.js,.ts,.html,.css"

# ChromaDB Configuration
CHROMADB_COLLECTION_NAME="csa_collection"
CHROMADB_PERSIST_DIRECTORY="data\\chroma_db"
